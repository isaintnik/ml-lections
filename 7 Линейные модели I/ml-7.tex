\documentclass[14pt, fleqn, xcolor={dvipsnames, table}]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}
\usepackage{cancel}

\usepackage{tikz}                   
\usetikzlibrary{shadows}

% \usepackage{enumitem}
% \setitemize{label=\usebeamerfont*{itemize item}%
%   \usebeamercolor[fg]{itemize item}
%   \usebeamertemplate{itemize item}}

\graphicspath{{images/}}

\usetheme{Madrid}
\usecolortheme{seahorse}
\renewcommand{\CancelColor}{\color{red}}

\setbeamercolor{footline}{fg=Blue!50}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    И. Кураленок, Н. Поваров
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Санкт-Петербург, 2016
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Стр. \insertframenumber{} из \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\newcommand\indentdisplays[1]{%
     \everydisplay{\addtolength\displayindent{#1}%
     \addtolength\displaywidth{-#1}}}
\newcommand{\itemi}{\item[\checkmark]}

\title{Линейные модели: введение\\\small{по материалам "The Elements of Statistical Learning"}}
\author[]{\small{%
И.~Куралёнок,
Н.~Поваров}}
\date{}

\begin{document}

\begin{frame}
\maketitle
\small
\begin{center}
\vspace{-60pt}
\vspace{80pt}
\footnotesize СПб, 2016
\end{center}
\end{frame}

\AtBeginSection[]
{  
 \addtocounter{framenumber}{-1}
 \begin{frame}<beamer>
   \frametitle{План}
   \tableofcontents[currentsection]
 \end{frame}
}

\begin{frame}{План}
\tableofcontents
\end{frame}

\begin{frame}{Формальная постановка}
Ищем решающую функцию в виде:
$$
y = F(\beta, x) = \beta^T x
$$
В $x$ можно добавить нулевым элементом $1$, тогда не надо таскать свободный член. \\
Такое решение кажется примитивным! \\
\uncover<2>{\bfДо того как мы расскажем что такое $x$.}
\end{frame}

\section{Линейная регрессия}
\begin{frame}{Простое решение}
$$
\arg \min_{\beta} \|F(X, \beta) - y\| = \arg \min_{\beta} \|X \beta - y\|
$$
Если норма $l_2$, то:
$$
{\partial T \over \partial X} = 2 X^T\left(X\beta - y\right) = 0
$$
$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$
\end{frame}

\begin{frame}{Геометрическая интерпретация}
\small
Если посмотреть на колонки, соответствующие фичам то картинка такая:
\begin{center}
\includegraphics[height=0.4\textheight]{3_2.png}
\end{center}
Об этом говорит (если нам все удалось):
$$
X^T(y - \hat{y}) = X^T(y - X\hat{\beta}) = 0
$$
В случае, если $rank(X) < n$ ортогональность остается!
\end{frame}

\subsection{Статистические свойства решения}

\begin{frame}{Вероятностная интерпретация линейной регрессии}
Предположим, что $y$ нам дан со случайной ошибкой $\epsilon \sim N(0, \sigma^2)$, тогда регрессию можно свести к правдоподобию:
$$
p(y|x, \beta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{\frac{-(y - \beta^T x)^2}{\sigma^2}}
$$
Тогда правдоподобие всей выборки выглядит так:
$$
\arg \max_\beta \log p(\mathbf{y}|X, \beta) = \arg \min_\beta \sum_i \left(y_i - \beta^Tx_i\right)^2
$$
значит можно по этой модели использовать ТВиМС для анализа $\beta$.
\end{frame}

\begin{frame}{Статистические свойства решения}
Если наблюдения независимы, $Var(y) = const$, а $x$ вычислены точно:
$$
Var(\hat{\beta}) = \left(X^TX\right)^{-1}\frac{1}{m - n - 1}\|y - y^*\|_2
$$
А если еще и предположить, что $y={\beta^*}^Tx + \epsilon$ и $\epsilon \sim N(0,\sigma^2)$:
$$
\hat{\beta} \sim N(\beta^*, \left(X^TX\right)^{-1}\sigma^2)
$$
а наблюдаемая $\sigma$ для $y$ распределена по $\chi^2$:
$$
(n-m-1)\hat{\sigma} = \|y - y^*\|_2 \sim \sigma \chi^2_{m-n-1}
$$
\end{frame}

\begin{frame}{А точно $\beta^*_i \ne 0$?}
Введем такую штуку ($Z$-score):
$$
z_i = \frac{\hat{\beta}_i}{\hat{\sigma}\sqrt{v_i}}
$$
где $v_i$ --- диагональный элемент $\left(X^TX\right)^{-1}$. Если подумать что $\beta^*_i = 0$, то:
$$
z_i \sim T_{m-n-1}
$$
Чем больше $Z$-score, тем более мы уверены, что $\hat{\beta}_i \ne 0$
\end{frame}

\subsection{Теорема Гаусса-Маркова}
\begin{frame}{Теорема Гаусса-Маркова I}
\begin{itemize}
  \item Независимость компонент $x$
  \item $X$ вычислены точно
  \item Распределение ошибки -- нормально с ожиданием $0$ (можно смягчить)
\end{itemize}
$\Rightarrow$
\begin{enumerate}
  \item Несмещенность
  \item Эффективность (лучше решения по разбросу не бывает)
  \item Состоятельность (чем больше данных, тем меньше разброс, асимптотика в $0$)
\end{enumerate}
\end{frame}

\begin{frame}{Теорема Гаусса-Маркова II}
\begin{theorem}[]
Линейное приближение по MSE обладает наименьшим разбросом из всех несмещенных линейных решений
\end{theorem}
\begin{itemize}
\item[$\Rightarrow$] для того, чтобы сделать решение более стабильным надо вводить bias
\item[$\Rightarrow$] простым MSE нам не отделаться, надо будет менять $T$
\end{itemize}
\end{frame}



\subsection{Расширение на несколько целевых функций}
\begin{frame}{Расширение на несколько целей}
$$
y \in \mathbb{R}^k
$$
В этом случае задача превращается в такую:
$$
\arg \min_B tr\left((Y - XB)^T(Y - XB)\right)
$$
$$
B_0 = \left(X^TX\right)^{-1} X^TY
$$
Если же $y = x^TB + \epsilon$, $\epsilon \sim N(0, \Sigma)$:
$$
\arg \min_B \left((Y - XB)^T\Sigma^{-1}(Y - XB)\right)
$$
\end{frame}

\section{Линейная классификация}
\begin{frame}{Классификация}
$$
x \in \mathbb{R}^n, y \in \{1,\ldots, c\}
$$
\begin{center}
\includegraphics[height=0.4\textheight]{4_1.png}
\end{center}
\small
Введем дискриминационные функции для каждого класса. У какого класса больше, тот и молодец. Там где равны --- границы решения.\\
NB: монотонные преобразования дискриминационным функциям не страшны
\end{frame}

\subsection{Простая линейная модель и ее вариации}
\begin{frame}{Линейное решение задачи классификации}
Можем пойти по-простому и решить регрессией:
$$
\gamma_{ij} = \left\{\begin{array}{ll}
1,& j = y_j \\
0 
\end{array}\right.
$$
В терминах предсказания $\gamma$ решаем:
$$
\arg \min_B tr\left((\Gamma - XB)^T(\Gamma - XB)\right)
$$
Среди предсказанных $xB$ выбираем максимум.
\end{frame}

\begin{frame}{Сложности с простым решением}\small
\begin{center}
\includegraphics[width=0.8\textwidth]{4_2.png}
\end{center}
\end{frame}

\subsection{Линейный дискриминантный анализ (LDA)}
\begin{frame}{Линейный дискриминантный анализ (LDA)}
Представим себе, что точки порождены смесью нормальных распределений по одному на класс:
$$
p(j|x) = \frac{1}{\sqrt{(2\pi)^n|\Sigma_j|}}e^{-\frac{1}{2}(x - \mu_j)^T\Sigma^{-1}_j(x - \mu_j)} \frac{p(j)}{p(x)}
$$
Если зафиксировать $\Sigma$, то границы между классами ($d_{jk} : \frac{p(j|x)}{p(k|x)} = 1$) --- прямые!
\small
$$
d_{jk}(x) = x^T\Sigma^{-1}(\mu_j - \mu_k) - \frac{1}{2}(\mu_j + \mu_k)^T\Sigma^{-1}(\mu_j - \mu_k) + \log\frac{p(j)}{p(x)}
$$
\end{frame}

\begin{frame}{LDA в пространсве $x$}
\begin{center}
\includegraphics[width=0.9\textwidth]{4_5.png}
\end{center}
\end{frame}

\begin{frame}{Аналитическое решение LDA}
Можно решать напрямую:
$$\begin{array}{c}
p(j) = \frac{m_j}{m},\\
\mu_j = \frac{1}{m_j}\sum_{x_u:y_u = j} x_u,\\
\Sigma = \frac{1}{m-k}\sum_{j=1}^k\sum_{x_u:y_u = j}(x_u - \mu_j)(x_u - \mu_j)^T \\
\end{array}$$
Можно $EM$ даже не заморачиваясь одинаковыми $\Sigma_j$
\end{frame}
\begin{frame}{Свойства LDA}
\begin{itemize}
  \item Нормальные распределения в основе
  \item Решение в аналитическом виде
  \item Работает даже в далеких от ``гауссовых'' ситуаций
  \item Имеет расширение в квадратичные мономы (QDA)
  \item Часто рассматривают диагональные $\Sigma_k$ для ускорения вычислений
  \item Можно использовать $\Sigma_k = \alpha\Sigma_0 + (1 - \alpha)\Sigma^k$
\end{itemize}
\end{frame}

\subsection{Логистическая регрессия}

\begin{frame}{Логистическая регрессия}
Будем искать не дискриминантные функции, а напрямую границы между классами:
$$
0 = log\left(\frac{p(j|x)}{p(k|x)}\right) = x^T \beta_{u}
$$
Преобразование справа --- logit. Тогда вероятности можно найти так:
$$
p(j|x) = {e^{x^T \beta_{j}} \over 1 + \sum_{v < c} e^{x^T\beta_v}}, j < c
$$
$$
p(c|x) = {1 \over 1 + \sum_{v < c} e^{x^T\beta_v}}
$$
\end{frame}

\begin{frame}{Оптимизация логистическая регрессия}
Вероятности у нас есть, давайте максимизировать правдоподобие!
$$
\arg \max_B \prod_i p(y_i|x_i, B)
$$
$$
= \arg \max_B \sum_i log {e^{x^T \beta_{y_i}} \over 1 + \sum_{v < c} e^{x^T\beta_v}}
$$
Как будем искать?
\end{frame}

\begin{frame}{Оптимизация логистической регрессии}
Сумма выпуклых --- выпукла, будем смотреть на каждую точку в отдельности.
$$
\log {e^{x^T\beta_y} \over 1 + \sum_{v} e^{x^T\beta_v}}
$$
$$
x^T\beta_y - \log \left(1 + \sum_{v} e^{x^T\beta_v}\right)
$$
Левая часть -- выпукла, правая часть --- комбинация выпуклой и линейной, выпукла. Градиентный спуск спасет отцов!
\end{frame}

\begin{frame}{Когда что?}
\begin{itemize}
  \item Есть много точек, для которых нет оценок $\Rightarrow$ LDA
  \item Есть подозрение на близость к норамльности $\Rightarrow$ LDA
  \item Хотим использовать prior $\Rightarrow$ LDA
  \item Во всех остальных случаях логистическая регрессия, особенно если есть много outlier'ов
\end{itemize}
\end{frame}

\begin{frame}{Какое $x$ бывает}
Просто фичи:
$$\begin{array}{l}
x \in \mathbb{R}^n \\
\end{array}$$
Мономы:
$$\begin{array}{l}
u \in \mathbb{R}^n \\
x = \prod u_j
\end{array}$$
Произвольные функции:
$$\begin{array}{l}
u \in \mathbb{R}^n \\
x: \mathbb{R}^n \to \mathbb{R}
\end{array}$$
В любом случае мы всегда можем посчитать значение $x$ по входным параметрам (например $u$).
\end{frame}


\section{Домашнее задание}

\begin{frame}{Домашнее задание}
\begin{itemize}
\item SVN, howto.txt
\item Дедлайн 31 октября
\item Мы надолго с этим датасетом
\end{itemize}
\end{frame}
\end{document}
