\documentclass[14pt, fleqn, xcolor={dvipsnames, table}]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}

\usepackage{tikz}                   
\usetikzlibrary{shadows}

% \usepackage{enumitem}
% \setitemize{label=\usebeamerfont*{itemize item}%
%   \usebeamercolor[fg]{itemize item}
%   \usebeamertemplate{itemize item}}

\graphicspath{{images/}}

\usetheme{Madrid}
\usecolortheme{seahorse}

\setbeamercolor{footline}{fg=Blue!50}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    И. Кураленок
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Санкт-Петербург, 2017
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Стр. \insertframenumber{} из \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\newcommand\indentdisplays[1]{%
     \everydisplay{\addtolength\displayindent{#1}%
     \addtolength\displaywidth{-#1}}}
\newcommand{\itemi}{\item[\checkmark]}

\title{Машинное обучение: начало\\\small{}}
\author[]{\small{И.~Куралёнок}}
\date{}

\begin{document}

\begin{frame}
\maketitle
\small
\begin{center}
\vspace{-60pt}
\vspace{80pt}
\footnotesize СПб, 2017
\end{center}
\end{frame}

\section{Формальное описание курса}

\begin{frame}
\frametitle{Что нужно, чтобы понять?}
\begin{itemize}
	\item ТВ и МС
	\item Линейная алгебра
	\item Язык программирования
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Как отчитываться?}
\begin{itemize}
	\item Экзамен как в универе
	\item Домашняя работа, которая дает бонусы на экзамене
	\item Статья с ``хорошей'' конференции/журнала
\end{itemize}
\end{frame}

\begin{frame}{Какие у нас цели?}
\begin{itemize}
	\item Уметь сформулировать задачу в терминах ML
	\item Найти подходящий класс решающих алгоритмов по формулировке
	\item Ориентироваться в области и знать ``где посмотреть'' существующие решения
	\item Понимать границы применимости
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Что почитать?}
\begin{itemize}
 	\item Википедия (лучше en)
\end{itemize}
\begin{itemize}
 	\item T. Hastie, R. Tibshirani, J. Friedman “The elements of Statistical Learning”
 	\item T. Mitchell “Machine Learning”
\end{itemize}
\begin{itemize}
 	\item Труды конференций: NIPS, ICML, CIKM, KDD, etc.
 	\item Журналы: JML, JMLR, JIS, NC, etc.
 	\item Видео курсы (нет им числа)
\end{itemize}
\end{frame}

\section{Введение}

\begin{frame}
\frametitle{Чтобы решить задачку обучения}
Нужно:
\only<2>{
	\begin{enumerate}
	\item Скачать данные
	\item Придумать метрику
	\item Запустить XGBoost
	\end{enumerate}
}
\uncover<3>{
	\begin{enumerate}
		\item Придумать цель и найти данные
		\item Обеспечить наблюдаемость цели в рамках описания
		\item Найти класс решающих функций подходящий для задачи
		\item Составить целевую функцию, близкую к предметной цели и ``простую в оптимизации''
		\item Провести оптимизацию и убедиться в том, что ответ достигает поставленных целей
	\end{enumerate}
}
\end{frame}

\begin{frame}{Чего (пока) не умеет ML}{Зачем мы все еще нужны :)}
\begin{enumerate}
	\item Ставить задачу и разбивать ее на этапы
	\item Моделировать ненаблюдаемые ситуации
	\item Переиспользовать накопленные знания для решения новых задач
	\item Понять полезность результата
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Data Mining vs. Machine Learning}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}}
\rowcolor{Blue!20}
& \small Data Mining & \small Machine Learning \\
\hline 
\bf Цель дисциплины & Выявление ``скрытых данных'' & Оптимизация целевой функции \\		
\bf Исследования & Больше про данные & Больше про методы \\
\bf Типичный результат & ``Мы применили такой метод и получили клевые результаты на таких стандартных данных'' & ``Предложили новый метод, который работает круче чем другие на нескольких датасетах (возможно даже синтетика)'' \\
\bf Где почитать & SIGIR, WSDM, WWWC, $\ldots$ & ICML, CIKM, $\ldots$ \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Artificial Intelligence vs. Machine Learning}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}}
\rowcolor{Blue!20}
& \small Artificial Intelligence & \small Machine Learning \\
\hline 
\bf Цель дисциплины & Рациональное поведение умных машин & Оптимизация целевой функции \\		
\bf Исследования & Больше про мат. моделирование & Больше про методы \\
\bf Типичный результат & ``Мы придумали как формализовать задачу игры в шахматы, применили такие методы и обыграли человека'' & ``Предложили новый метод, который работает круче чем другие на нескольких датасетах (возможно даже синтетика)'' \\
\bf Где почитать & AAAI, IJCAI, $\ldots$ & ICML, CIKM, $\ldots$ \\
\end{tabular}
\end{center}
\end{frame}

\subsection{Определения}

\begin{frame}
\frametitle{Машинное обучение: определения (I)}
\begin{quote}
A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
\begin{flushright}\textbf{Tom~M.~Mitchell}\end{flushright}
\end{quote}
\end{frame}

\begin{frame}
\frametitle{Машинное обучение: определения (II)}
\begin{quote}
Machine learning --- the ability of a machine to improve its performance based on previous results.
\begin{flushright}\textbf{Webster}\end{flushright}
\end{quote}
\end{frame}

\begin{frame}
\frametitle{Машинное обучение: определения (III)}
\begin{quote}
Машинное обучение —-- обширный подраздел искусственного интеллекта, изучающий методы построения алгоритмов, способных обучаться.
\begin{flushright}\textbf{ru.wikipedia.org}\end{flushright}
\end{quote}
\end{frame}

% Вывод такой: нету единого понимания о чем все это. Также нету структуры, которая была бы стандартной,
% поэтому дальше мы придумали свою собственную.

\subsection{Минутка истории}
\begin{frame}
\frametitle{Машинное обучение: немного истории}
\begin{small}
\begin{description}
	\item[50-70гг] базы знаний, полнотекстовый поиск, распознавание образов, нейронные сети
	\item[70-80гг] символьный вывод, Quinlan ID3 деревья, разумные практические результаты, VC-оценки
	\item[80-90гг] первые конференции, много практического применения, активное применение кластеризации в анализе
	\item[90-00гг] повторное сэмплирование в ML, SVM, применение в IR, ML != DM, LASSO, bootstrap, bagging, boosting
	\item[00-10гг] Compressed sensing и прочие восстановления сигналов, царство деревьев, развитие ансамблей, $\ldots$
	\item[10-] Deep learning $\ldots$
\end{description}
\end{small}
\end{frame}

\subsection{Понятия и обозначения}
\begin{frame}
\frametitle{Основные понятия}
\begin{itemize}
	\item Область работы $=$ Universe $=\Gamma$.
	\item Решающая функция $=$ Decision Function $=$ $\hat{F} \in F$ -- класс решающих функций.
	\item Опыт $=$ Data Set $=$ $D = X \times Y$.
	\item Целевая функция $=$ Target $=T(y,F(x))$.
\end{itemize}
\end{frame}

\begin{frame}{Задача обучения}
В ML оптимизация часто проводится в одних условиях, а эксплуатация в других. 
$$
\arg \max_{F, B: \hat{F} = B(F)} A(\Gamma, \hat{F})
$$
\begin{itemize}
	\item $A$ --- цели эксплуатации (например деньги) на всей области работы $\Gamma$
	\item $B$ --- способ оптимизации, который реализуем
\end{itemize}
\end{frame}

\begin{frame}{Как устроена эксплуатация}{На самом деле, как она \_может\_ быть устроена}
Будем считать, что работа на разных элементах $\Gamma$ независима. Тогда эксплуатацию $A$ можно представить так:
$$
A=\mu_{x \sim U(\Gamma)} T_A(\hat{F}(x))
$$
\end{frame}

\begin{frame}{Как устроена оптимизация}{На самом деле, как она \_может\_ быть устроена}
Способ оптимизации $B$, в свою очередь, такая штука:
$$
\hat{F} = B(F, D) = \arg \max_{F} T_B(D, F) = \arg \max_{F} T_B(Y,F(X));
$$

В рамках этой лекции, мы не будем рассматривать разницу между $T_A$ и $T_B$, поэтому будем считать, что они одинаковы $T$.
\end{frame}

\subsection{Классификация методов ML}
\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item способу получения опыта;
	\item виду целевой функции;
	\item классу решающих функций.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item {\bfспособу получения опыта;}
	\item виду целевой функции;
	\item классу решающих функций.
\end{itemize}
\end{frame}

\subsubsection{В зависимости от способа получения опыта}
\begin{frame}
\frametitle{Классификация машинного обучения: опыт}
\begin{itemize}
	\item Transductive learning
	\item Обычное обучение
	\item Стохастическая оптимизация (stochastic optimization)\footnote{По сути это не обучение, но очень похоже.}
	\item Активное обучение (active learning)
	\item Обучение с бюджетом (budget learning) 
	\item Интерактивное обучение (online learning)
	\item Многорукие бандиты (multi-armed bandits)
	\item Обучение с подкреплением (reinforcement learning) 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Transductive learning}
\begin{enumerate}
	\item Все множество, на котором работаем задано перечислением $\Gamma_0$
	\item Для части данных известен ответ, и они формируют $D$
\end{enumerate}
$$\begin{array}{l}
\hat{F}=B(F, D, \Gamma_0) \\
A(\Gamma_0, \hat{F})
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Обычное обучение}
\begin{enumerate}
	\item Фиксируем множество примеров $X$
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на доступных примерах, используя информацию о всех
\end{enumerate}
$$\begin{array}{l}
\hat{F} = B(F, D) \\
A(\Gamma, \hat{F})
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Стохастическая оптимизация\footnote{Одна из возможных версий предмета :).}}
\begin{enumerate}
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на следующем примере пока не надоест (до момента $T$)
\end{enumerate}
$$\begin{array}{l}
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \left(x_t \sim U(\Gamma)\right) \\
\hat{F}_t=B(F, D_t) \\
A(\hat{F}_T, \Gamma)
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Активное обучение}
\begin{enumerate}
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на всех доступных примерах
	\item Пополняем множество примеров по просьбе алгоритма $\mathcal{A}$ и переходим к п.~2, если $\mathcal{A}$ не требует больше данных останавливаемся в $T$
\end{enumerate}
$$\begin{array}{l}
D_{t+1} = D_{t} \cup \{\mathcal{A}(D_t, \hat{F}_t)\} \\
\hat{F}_t = B(F, D_{t}) \\
A(\hat{F}_T, \Gamma)
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Обучение с бюджетом}
\begin{enumerate}
	\item Введем стоимость получения информации по точке $c(d)$ и бюджет обучения $B$
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на всех доступных примерах
	\item Пополняем множество примеров по просьбе алгоритма $\mathcal{A}$, пока не закончился бюджет $\sum_t c({x_t, y_t}) < C$ и переходим к п.~3
\end{enumerate}
$$\begin{array}{l}
D_{t+1} = D_{t} \cup \{\mathcal{A}(D_t, \hat{F}_t)\} \\
\hat{F}_t = B(F, D_{t}) \\
A(\hat{F}_T, \Gamma)
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Интерактивное обучение (online)}
\begin{enumerate}
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на всех доступных примерах
	\item Получаем следующую точку, исходя из работы решающей функции и переходим к п.~2
\end{enumerate}
$$\begin{array}{l}
A(\hat{F}, \Gamma) = \sum_{t} A(\hat{F}_t, x_t) \\
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \{(x_{t+1})\} \\
\hat{F}_t = B(F, D_{t}) \\
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Многорукие бандиты}
\footnotesize
\begin{enumerate}
	\item Фиксируем множество возможных действий $M$, $m \in M$.
	\item Каждое действие ведет к ответу $y \sim \xi(x)^*$.
	\item Получаем следующую точку $x_t$
	\item Обновляем модель $\hat{F}_t = \{\xi_{tm}, m \in M$.
	\item Выбираем следующее действие исходя из алгоритма $\mathcal{A}(\hat{F}_t, x)$.
	\item Получаем по действию ответ среды
	\item Повторяем с п.3 пока не надоест.
\end{enumerate}
$$\begin{array}{l}
A(\hat{F}, \Gamma) = \sum_{t} A(y_t = y \sim \xi_{m_t = \mathcal{A}(\hat{F}_t, x_t)}, x_t) \\
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \left\{\left((x_{t+1}, y_{t+1}), m_{t+1}\right)\right\} \\
\hat{F}_t = B(F, D_t) \\
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Обучение с подкреплением\footnote{На самом деле, это уже AI.}}
\footnotesize
\begin{enumerate}
	\item Фиксируем множество возможных действий $M$, ``поле'' $X$, множество возможных ответов среды $S = S^o \cup S^n$.
	\item Получим наблюдаемую часть ответа среды на текущее состояние.
	\item Обновим модель предсказания $\hat{F}_t = S(m)$
	\item Сделаем следующий ход исходя из алгоритма $\mathcal{A}(\hat{F}_t, s^o)$.
	\item Повторяем с п.2 пока не надоест или среда нас не убъет.
\end{enumerate}
$$\begin{array}{l}
A(\hat{F}, M, X, S) = \sum_t A(S_t(m_t = \mathcal{A}(\hat{F}_t, x_t)), {S_i}_1^t) \\
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \left\{\left((x_{t+1}, y_{t+1} \in S^o), m_{t+1}\right)\right\} \\
\hat{F}_t = B(F, D_t) \\
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item способу получения опыта;
	\item {\bfвиду целевой функции;}
	\item классу решающих функций.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения: цель}
\begin{itemize}
	\item С учителем
	\begin{itemize}
		\item классификация (classification);
		\item аппроксимация (regression);
		\item отношение порядка (learning to rank);
		\item обучение метрики (metric learning).
	\end{itemize}
	\item Без учителя:
	\begin{itemize}
		\item кластеризация (cluster analysis);
		\item уменьшение размерности (dimensionality reduction);
		\item обучение отображению (representation learning).
	\end{itemize}
	\item Смешанные:
	\begin{itemize}
		\item условная кластеризация;
		\item transfer learning.
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[t]\frametitle{Обучение с учителем I}
\begin{description}
	\item[Классификация:] в качестве метрики LL, KL, CE, etc.
	$$\begin{array}{l}
	y \in \left\{-1,1\right\}, F: \Gamma \to \left[0,1\right] \\
	y \in \left\{1,\ldots, m\right\}, F: \Gamma \to \left[0,1\right]^m \\
	y \in \left\{0, 1,\ldots, m\right\}, F: \Gamma \to \left[0,1\right]^{m + 1}\\
	\end{array}$$ 
	\item[Аппроксимация:] варианты MSE.	$y \in \mathbb{R}$, $F: \Gamma \to \mathbb{R}$
\end{description}
\end{frame}
\begin{frame}[t]\frametitle{Обучение с учителем II}
\begin{description}
	\item[Отношения порядка:] цель зависит от модели, но почти всегда хотим найти $F: \Gamma \to \mathbb{R}$:
	\begin{itemize}
	{\setlength{\itemindent}{-4em}
		\item pointwise: $x \in \Gamma, y \in \mathbb{R}, T$ -- MSE; 
		\item pairwise: $x \in \Gamma^2, y \in \left\{<,=,>\right\}, T$ -- см. классификацию;
		\item listwise: $x \in \Gamma^n, y \in ?, T$ -- специфичен для конкретного применения.
	}
	\end{itemize}
\end{description}
\end{frame}
\begin{frame}[t]\frametitle{Обучение с учителем III}
\begin{description}
	\item[Обучение метрики:] хотим построить такую функцию от пары чтобы:
	\begin{itemize}
	{\setlength{\itemindent}{-4em}
		\item Она отражала заданную семантику
	}
	{\indentdisplays{-5em}
		$$\begin{array}{l}
		X=\Gamma^2,\\
		Y = \{(a,b,c,d): m(a,b) < m(c,d), a,b,c,d \in \Gamma\}, \\
		F: \Gamma^2 \to \mathbb{R}
		\end{array}$$
	}
	{\setlength{\itemindent}{-4em}
		\item По возможности была метрикой
	}
	\end{itemize}
\end{description}
Целевая функция обычно основывается на классификации
\end{frame}

\begin{frame}[t]\frametitle{Обучение без учителя I}
\begin{description}
	\leftmargin=-5em
	\itemindent=0em
	\labelwidth=0em
	\leftskip=-5em
	\item[Уменьшение размерности:] надо отобразить исходное пространство в пространство меньшей размерности, максимально сохранив заданные свойства.
	\begin{itemize}
	\leftskip=-5em \small
		\item расстояние: $x,y \in \Gamma, ||F(x) - F(y)|| - ||x - y||$;
		\item статистику: $\{x_i\}_1^n \in \Gamma^n, \Psi(\{x_i\}_1^n) - \Psi(\{F(x_i)\}^n_1)$;
	\end{itemize}
	\item[Кластеризация:] это такое уменьшение размерности до ``упора'', в качестве статистики, которую надо оставить выступает ``чувство прекрасного''. Например: {\indentdisplays{-5em} \small $$
	\Psi = \left\{\begin{array}{l}0, ||x - y|| < \epsilon\\1\end{array}\right.
	$$}
	\item[Обучение представлению] тоже уменьшение размерности, но ограничения накладываются уже на то как видимы результаты.
\end{description}
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item способу получения опыта;
	\item виду целевой функции;
	\item {\bfклассу решающих функций.}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Основные классы решающих функций}
\begin{itemize}
\item Линейные решения
\item Параметрические семейства функций
\item Графы
\item Нейронные сети (ANN)
\item Instance based learning (kNN)
\item Предикаты
\item Ансамбли
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (I)}
\begin{itemize}
\item Линейные решения 
	\begin{itemize}
	\item Линейная регрессия, логистическая регрессия
	\item Скрытый дискриминантный анализ (LDA/QDA*)
	\item LASSO
	\item SVM
	\item LSI*
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (II)}
\begin{itemize}
\item Графы
\begin{itemize}
	\item Марковские модели (цепи, HMM)
	\item Графические модели
	\item Conditional Random Fields
\end{itemize}
\item Нейронные сети (ANN)
\begin{itemize}
	\item Персептронные сети
	\item Сети Хопфилда++
	\item Сети Кохоннена
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (III)}
\begin{itemize}
\item Параметрические семейства функций
	\begin{itemize}
		\item Сэмплирование
		\item Генетические алгоритмы
		\item PLSI/LDA (Latent Dirichlet Allocation)/прочие модели с распределениями (несть им числа)
	\end{itemize}
\item Instance based learning (kNN)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (IV)}
\begin{itemize}
\item Предикаты
	\begin{itemize}
	\item Логические выражения
	\item Деревья решений
	\end{itemize}
\item Ансамбли
	\begin{itemize}
	\item Просто ансамбли 
	\item Bagging
	\item Boosting
	\item BagBoo/BooBag
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}\huge
Вопросы?
\end{center}
\end{frame}

\renewcommand{\arraystretch}{1.5}

\begin{frame}
\frametitle{Дедуктивные/индуктивные методы}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.45\textwidth}|p{0.45\textwidth}}
\rowcolor{Blue!20}
\small Индуктивные & \small Дедуктивные \\
\hline 
Полагаются на статистику & Полагаются на prior knowledge \\ 
Используют классы элементарных функций & Решающая функция следует из предполагаемой структуры \\
Работают в любой области & Привязаны к области \\ 
Знание области отражается на составление target & Понимание области меняет решающую функцию \\
Логистическая регрессия & LDA \\
&\\
\bf \em Для вхождения в область, при больших размерностях & \bf \em Небольшие размерности, «давно тут сидим» \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Применение методов ML}
\begin{itemize}
\item Практически везде (дайте задачку, я попробую придумать применение)
\item Есть два больших класса работ
\end{itemize}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}}
\rowcolor{Blue!20}
& \small Академические & \small Практические \\
\hline 
\bf Цели & Существуют ситуации, когда работает хорошо & Обеспечивает измеряемое качество на множестве примеров \\
\bf Искать & Красивые идеи, хорошую математику & Работающие вещи, много грязных приемов \\
\bf Смотреть & Конференции & Соревнования \\
\end{tabular}
\end{center}
\end{frame}

\end{document} 
