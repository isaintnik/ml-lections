\documentclass[14pt, fleqn, xcolor={dvipsnames, table}]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}
\usepackage{cancel}
\usepackage{graphicx}
\usepackage{animate}

\usepackage{tikz}
% \usepackage{enumitem}
\usetikzlibrary{shadows}

% \usepackage{enumitem}
% \setitemize{label=\usebeamerfont*{itemize item}%
%   \usebeamercolor[fg]{itemize item}
%   \usebeamertemplate{itemize item}}

\graphicspath{{images/}}

\usetheme{Madrid}
\usecolortheme{seahorse}
\renewcommand{\CancelColor}{\color{red}}

\setbeamercolor{footline}{fg=Blue!50}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    И. Кураленок
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Санкт-Петербург, 2016
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Стр. \insertframenumber{} из \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\newcommand\indentdisplays[1]{%
     \everydisplay{\addtolength\displayindent{#1}%
     \addtolength\displaywidth{-#1}}}
\newcommand{\itemi}{\item[\checkmark]}

\newenvironment{mydescription}[1]
  {\begin{list}{}%
   {\renewcommand\makelabel[1]{\color{blue}##1:\hfill}%
   \settowidth\labelwidth{\makelabel{#1}}%
   \setlength\leftmargin{\labelwidth}
   \addtolength\leftmargin{\labelsep}}}
  {\end{list}}

\title{Уменшение размерности: обзор FE\\\small{}}
\author[]{\small{%
И.~Куралёнок}}
\date{}
\begin{document}

\begin{frame}
\maketitle
\small
\begin{center}
\vspace{-60pt}
\vspace{80pt}
\footnotesize СПб, 2017
\end{center}
\end{frame}
\section{Чем feature extraction отличается от развешивания датчиков?}

\begin{frame}{Как отличить extraction от датчиков?}{Пример с мобильниками}
Как понять каким образом человек передвигается?\\
{\color{blue} Датчики:} повесим специальный датчик \\
{\color{blue} FE:} соберем данные от пользователей и спомощью существующих датчиков предскажем.
\end{frame}

\begin{frame}{Какие есть способы собрать что-то удобное?}
\small
\begin{itemize}
  \item Без использования дополнительных данных (какая-то нормализация)
  \begin{itemize}
    \footnotesize
    \item с точки зрения линейной алгебры (PCA/Kernel PCA)
    \item с точки зрения разделения сигналов (ICA)
    \item группировка (кластеризация)
    \item какие-то еще принципы, специфичные для данных (например поделить голосовой сигнал)
  \end{itemize}
  \item С дополнительными данными
  \begin{itemize}
    \footnotesize
    \item Metric learning
    \item Хитрая кластеризация с предпочтениями
    \item Transfer learning
  \end{itemize}
  \item Пускай само думает
  \begin{itemize}
    \footnotesize
    \item Deep learning
  \end{itemize}
\end{itemize}
\end{frame}

\section{PCA}

\begin{frame}{Метод главных компонент}{Из определения}
{\color{blue}Задача:} превратить множество скоррелированных сигналов в множество линейно некоррелирующих с помощью поворота:
$$\begin{array}{l}
\min \sum_{(i,j)} y_i^T y_j \\
X = Q^T Y Q \\
q_i^T q_j = 0, \forall i \ne j \\
q_i^T q_i = 1, \forall i 
\end{array}$$
Мы знаем хотя бы одно решение такой задачи! При этом не все строки в $Y$ одинаково полезны. Отсортируем их по модулю.
\end{frame}

\begin{frame}{Метод главных компонент}{Альтернативный способ: отсосы}
Попробуем итеративно выбирать направление, несущее наибольшую ``информацию'' (на самом деле дисперсию):
$$\begin{array}{l}
\arg \max_{\|w\|_2=1} \sum_i (x_i^Tw)^2 \\
\arg \max_{\|w\|_2=1} w^T X^TXw \\
\arg \max_{w} {w^T X^TXw \over w^Tw} \\
\end{array}$$
Это Rayleigh Quotient, для которого известно решение -- главный собственный вектор.
\end{frame}

\begin{frame}{Как реализовать PCA?}
\begin{itemize}
  \item Можно искать корни уравнения $det\|X - \lambda E\| = 0$, но мы так делать не будем
  \item Возведение в степень (power iteration)
  \item Тридиагонализация + (D \& C или что-то более модное)
  \item Последовательное LQ (QR) разложение
\end{itemize}
\end{frame}

\begin{frame}{Свойства PCA}
\begin{itemize}
  \item Все хорошо, когда пространство похоже на нормальное распределение. Однако на практике это обычно не так:
  \only<2>{\begin{center}\includegraphics[width=0.6\textwidth]{pca.png}\end{center}}
  \only<3->{\item Мы рассмотрели линейную зависимость, а что если она нелинейна?}
\end{itemize}
\uncover<4->{$\Rightarrow$ попробуем стандартный фокус с kernel}
\end{frame}

\section{Kernel PCA}
\begin{frame}{Kernel PCA}
\footnotesize
Хотим решить задачу PCA в каком-то другом пространстве, образованном из данного через преобразование $\Phi$. Для этого нам нужно лишь узнать проекции на собственные вектора $\Phi(x)^T v_k$. Для этого воспользуемся таким:
$$\begin{array}{l}
\lambda v = Cv \\
x^T v = \frac{1}{\lambda} x^T Cv \\
v = \sum a_i \Phi(x_i)
\end{array}$$
Все это добро выполняется для точек из $X$. Представим, что kernel space имеет размерность $N$.
$$
N \lambda K a = K^2 a
$$
Решив это уравнение, получим разложение любого $\Phi(x)$ по $V$
$$
\Phi(x)^Tv_k = \frac{1}{\lambda_k} \sum_i a_i K(x, x_i)
$$
\end{frame}

\section{ICA}
\begin{frame}{Общая схема ICA}
Представим сигнал как сумму независимых например так:
$$x = A (B(\alpha_i, \beta_i))_{i=1}^n$$
\only<1>{Теперь наша задача подобрать такие $(A, \alpha, \beta)$, которые будут наилучшим образом объяснять данные.}
\only<2->{Теперь наша задача подобрать такие $(A, \alpha, \beta)$, которые будут \textit{наилучшим образом} \textit{объяснять} данные.}
\uncover<3>{Это можно делать по тем же принципам что и PCA:
\begin{itemize}
  \item получше объяснить данные (ICA on LL/Infomax/KL/etc.);
  \item выделяя самые ``жирные'' направления по-одному (Projection Pursuit)
\end{itemize}}
\end{frame}

\begin{frame}{Общая модель и ее свойства}
\small
$$
x = As, s = Wx
$$
Генеративная модель с такими свойствами:
\begin{itemize}\footnotesize
  \item так как подбираем одновременно и $A$ и $s$, нужно зафиксировать свойства $s$ (ожидание и scale)
  \uncover<2->{\item $s$ не могут быть одинаково симметрично распределены (например нормально)}
  \only<3>{
  \begin{center}
  \includegraphics[height=0.4\textheight]{gaussian2.png}
  \end{center}
  }
  \uncover<4->{\item исходные сигналы $s$ никак не упорядочены}
\end{itemize}
\end{frame}

\begin{frame}{``Ненормальность'' и независимость}
Рассмотрим проекцию: $w^T x$, тогда:
$$
w^T x = w^T As = (A^T w)^T s = z^T s
$$
Тогда минимизация нормальности проекции приведет ее к $z = (0,\ldots,1,\ldots,0)^T$, который соответствует одному из искомых независимых направлений.

$\Rightarrow$ осталось понять как искать ненормальных
\end{frame}

\begin{frame}{Projection pursuit}
\small
Известно, что нормальное распределение имеет моменты старших порядков либо 0 либо 1. Поэтому можно поискать такие $w$, которые максимизируют отличия моментов от 0 или 1. После дисперсии там skewness и kurtosis. Которые отвечают за скрученность и остроконечность. Кубы никто не любит, так что обычно максимизируют kurtosis. Можно поставить такую задачу:
$$
Kurt(\xi)={\mu(\xi_i - \mu(\xi))^4\over \left(\mu(\xi_i - \mu(\xi))^2\right)^2}
$$
$$
\arg \max_w Kurt(w^T x)
$$

\end{frame}

\subsection{Через ``ненормальность''}

\begin{frame}{Ненормальность через Negentropy}
\small
$Kurt$ неустойчив к выбросам, поэтому на практике хочется использовать что-то другое. Хотим померить количество информации, которую нам дает ``ненормальность'':
$$
J(X|M(X)) = H(\mathcal{N}(\hat{\mu}(X),\hat{\sigma}(X))) - H(M(X))
$$
Так как нормальное распределение --- самое непонятное, то $J > 0$. $J$ тяжело считать напрямую, поэтому используют приближения:
$$
J(y) \simeq \frac{1}{12}\mu(y^3)^2 + \frac{1}{48}Kurt(y)^2
$$
классическое, но работает не очень, так как $Kurt$. 
$$
J(y) \simeq \sum_{i=1}^p (\mu(g_i(y^*)) - \mu(g_i(\nu))), \nu \sim \mathcal{N}(0,1), y^* = \Sigma^{-\frac{1}{2}} (y - \mu(y))
$$
\end{frame}

\begin{frame}{Метод FastICA}
\begin{enumerate}
  \item Нормализуем данные
  \item Выберем такую $f$, которые позволят нам быстро сходиться и будут далеки от квадратов
  \item Посчитаем от них производные $g$
  \item Возьмем произвольный $w$
  \item Минимизируем c помощью Ньютона
$$
\hat{w} = \arg \min_{\|w\| = 1} (\mathbb{E}(f(w^T x)) - \mathbb{E}(f(\nu)))
$$
  
\end{enumerate}
\end{frame}

\subsection{Через variational bayes}
\begin{frame}{Через variational Bayes}
Итак мы хотим максимизировать KL:
$$
\arg \max_{\Sigma,\mu} \int_\gamma KL(p(\gamma | X) \| p(\gamma | \mathcal{B}(\alpha, \beta)) d\gamma
$$
если развернуть все в likelihood, то получится так:
$$
\arg \max_{\Sigma,\mu} \int_\gamma KL(p(X|\gamma) \| p(\gamma | \mathcal{B}(\alpha, \beta)) d\gamma
$$
Наша задача из этого выитегрировать $\gamma$. Фишка в том, что размерность векторов $\alpha, \beta$, могут быть существенно меньше чем $\beta$
\end{frame}

\begin{frame}{Что есть еще}
Область очень горячая и есть еще много чего:
\begin{itemize}
  \item Auto encoders
  \item t-SNE
  \item Locally Linear Embedding
  \item etc.
\end{itemize}
\end{frame}

\begin{frame}{Locally Linear Embedding}
\end{frame}

\begin{frame}{Что мы сегодня узнали}
\begin{itemize}
  \item Чем мы занимаемся зависит от точки зрения: можно гордо делать датчики, а можно клепать костыли
  \item Если не использовать никакой дополнительной информации, то можно постараться нормировать модель тем или иным способом
\end{itemize}
\end{frame}

\end{document}
