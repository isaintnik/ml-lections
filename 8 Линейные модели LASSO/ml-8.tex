\documentclass[14pt, fleqn, xcolor={dvipsnames, table}]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}
\usepackage{cancel}

\usepackage{tikz}                   
\usetikzlibrary{shadows}

% \usepackage{enumitem}
% \setitemize{label=\usebeamerfont*{itemize item}%
%   \usebeamercolor[fg]{itemize item}
%   \usebeamertemplate{itemize item}}

\graphicspath{{images/}}

\usetheme{Madrid}
\usecolortheme{seahorse}
\renewcommand{\CancelColor}{\color{red}}

\setbeamercolor{footline}{fg=Blue!50}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    И. Кураленок
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Санкт-Петербург, 2017
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Стр. \insertframenumber{} из \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\newcommand\indentdisplays[1]{%
     \everydisplay{\addtolength\displayindent{#1}%
     \addtolength\displaywidth{-#1}}}
\newcommand{\itemi}{\item[\checkmark]}

\title{Автоматический feature selection на примере линейных моделей\\\small{}}
\author[]{\small{%
И.~Куралёнок}}
\date{}

\begin{document}

\begin{frame}
\maketitle
\small
\begin{center}
\vspace{-60pt}
\vspace{80pt}
\footnotesize СПб, 2017
\end{center}
\end{frame}

% \AtBeginSection[]
% {  
%  \addtocounter{framenumber}{-1}
%  \begin{frame}<beamer>
%    \frametitle{План}
%    \tableofcontents[currentsection]
%  \end{frame}
% }


\section{Variance линейных моделей слишком велик!}

\begin{frame}{Воспоминания о былом}
Сегодня будем искать решающую функцию в параметрическом семействе $h(x, \beta)$.
\begin{itemize}
  \item Будем рассматривать решение задачи оптимизации, как распределение над $\beta$
  \item Можно предположить, что при увеличении количества параметров линейные модели имеют большой разброс решений
  \item Из курса алгебры (?) мы знаем, что меньше разброс нам не сделать с избранной целевой функцией (теорема Гаусса-Маркова)
\end{itemize}
$\Rightarrow$ будем менять целевую функцию: вводить условия, или менять саму $T$.
\end{frame}

\section{Поиск условного оптимума}
\begin{frame}{Хорошие решения регрессии}
\begin{enumerate}
  \item Предположим, что данные $y$ y нас шумные
  \item Введем чувство прекрасного $-R(\beta)$ 
\end{enumerate}
Тогда нашу проблему можно свести к:
$$\begin{array}{l}
\arg \min_\beta R(\beta) \\
\| h(X,\beta) - y\| < \epsilon
\end{array}$$
Ну или так:
$$\begin{array}{l}
\arg \min_\beta \| h(X,\beta) - y\| \\
R(\beta) < p
\end{array}$$
\end{frame}

\begin{frame}{Преобразование целевой функции}
$$
\arg \min_\beta \| р(X, \beta) - y\| + \lambda R(\beta)
$$
Можно найти такой параметр $\lambda$, который будет давать решение задачи на предыдущем слайде. В смысле оптимизации проблемы эквивалентны.
\end{frame}


\begin{frame}{C байесовой точки зрения}
Строго говоря, мы ввели prior на распределение решений:
$$\begin{array}{l}
\arg \max_\beta P(y|X\beta) P(\beta) \\
= \arg \max_\beta \sum_i log P(y_i|x_i\beta) + log P(\beta)
\end{array}$$
Если предположить нормальность $P(y_i|x_i\beta)$, то проблема становится очень похожей на то, что мы уже видели:
$$
= \arg \min_\beta \|X \beta - y\| - z log P(\beta)
$$
где $z$ --- ошметки нормализации.
\end{frame}

\begin{frame}{Виды prior}
Какими бывают $-log P(\beta)$:
\begin{description}
  \item[$\|\beta\|_0$] --- бритва Оккама, MDL, etc.;
  \item[$\|\Gamma\beta\|_2$] --- нормальное распределение c $\Sigma = \Gamma^{-1}$ и $\mu = 0$;
  \item[$\|\beta\|_1$] --- распределение Лапласа;
\end{description}
Как можно видеть все это добро обобщается в $l_q$.
\end{frame}

\begin{frame}{Сравнение prior'ов в случае $l_1$ и $l_2$}
\begin{center}
\includegraphics[width=0.4\textwidth,height=0.5\textheight]{800px-Laplace_distribution.png} 
\includegraphics[width=0.4\textwidth,height=0.5\textheight]{800px-Normal_distribution.png} 
\end{center}
Картинки из википедии
\end{frame}

\subsection{Почему все это очень клево ложится на ML}
\begin{frame}{Как это относится к ML}
Точное решение в несет слишком много информации о обучающей выборке, поэтому точно оптимизировать смысла нету. Таким образом условие:
$$\begin{array}{l}
\min R(\beta) \\
\|h(X,\beta) - y\| < \epsilon
\end{array}$$
хорошо ложится на ML. При этом возможность выбора $R$ позволяет рассказать наши ожидания от структуры решения.
\end{frame}

\begin{frame}{Как это называется}
\begin{description}
  \item[$\|\beta\|_0$] --- Best Subset/Akaike information criterion;
  \item[$\|\Gamma\beta\|_2$] --- регуляризация Тихонова/ridge regression;
  \item[$\|\beta\|_1$] --- least absolute shrinkage and selection operator (LASSO);
\end{description}
Также рассматривается обощение на $l_q$.
\end{frame}

\subsection{Картинки разных условий}

\begin{frame}{Геометрия LASSO}
$$\begin{array}{l}
\min_\beta {\color{blue} \|X\beta - y\|} \\
{\color{red} \|\beta\|_1 < p}
\end{array}$$
\begin{center}
\includegraphics[width=0.9\textwidth]{Geometric_View.png} 
\end{center}
\footnotesize
Картинки из ICML 2010 Tutorial (Irina Rish, Genady Grabarnik)
\end{frame}

\begin{frame}{Геометрия LASSO II}
\small
Рассмотрим зависимость компоненты решения $\beta_0$ от решения безусловной системы $\hat{\beta} = \left(X^TX\right)^{-1}X^Ty$.
\begin{center}
\includegraphics[width=0.9\textwidth]{shrinkage.png} 
\end{center}
\footnotesize
Картинки из The Elements of Statistical Learning (Hastie, Friedman and Tibshirani, 2009)
\end{frame}

\section{Как считать}
\begin{frame}{Пара слов об оптимизации $l_0$}
\small
$l_0$ --- естественное условие на решение: ``меньше фишек --- легче думать'' \\
Точное решение однако найти очень дорого: проблема $NP$-сложная. Поэтому есть масса работ по тому как найти приближенное решение:
\begin{itemize}
  \item Если $n < 40$ есть работа ``Regressions by Leaps and Bounds'' (Furnival and Wilson 1974) + CV для выбора эффективного ограничения
  \item Взад-назад селекшен (Forward/Backward-Stepwise Selection): выбрасываем или добавляем фичу с наибольшим/наименьшим Z-score
  \item Приближение с маленьким $q$
  \item ``Хитрые'' переборы многомерного креста
\end{itemize}
\end{frame}

\begin{frame}{Оптимизация $l_1$ и $l_2$}
\small
В случае ridge regression все просто:
$$
\beta_0 = \left(X^{T}X+ \Gamma^{T} \Gamma\right)^{-1}X^{T}y
$$
А в случае LASSO все не так просто, так как $T$ негладкая. Поэтому там почти градиентный спускъ. Есть несколько способов:
\begin{itemize}
\small
  \item Пошаговый LASSO
  \item LARS
  \item Покоординатный спуск (посмотреть дома)
  \item ISTA
  \item FISTA
  \item etc.
\end{itemize}
\end{frame}

\subsection{Stagewise LASSO}
\begin{frame}{Пошаговый LASSO}
\begin{enumerate}
  \item Начнем с $\beta = 0$ и зафиксируем шаг $w$ и соответствующие ему шаги по всем ортам $e_iw$
  \item Выберем такое направление: 
  $$\begin{array}{rl}
  \hat{i} &= \arg \min_i \|y - X\left(\beta_t \pm e_iw\right)\|\\
          &= \arg \min_i \|\left(y - X\beta_t\right) \pm X e_iw\| \\
          &= \arg \min_i \|r_t \pm X e_iw\| \\
  \end{array}$$
  \item $\beta_{t+1}$ = $\beta_{t} \pm e_{\hat{i}}w$ если $\|y - X\beta_t\| - \|y - X\beta_{t+1}\| > \lambda$
\end{enumerate}
\end{frame}

\begin{frame}{Наблюдение за работой LASSO}
Несколько наблюдений:
\begin{itemize}
  \item Мы много раз бродили в одном и том же направлении
  \item За шаг двигаемся только по одной координате
  \item Так как шаг $w$ фиксирован приходим не в точное решение, а в приближенное
\end{itemize}
\end{frame}

\subsection{LARS}
\begin{frame}{Least angle regression (LARS)}
\small
Внимание, $1$ дальше --- это вектор нужной размерности.
\footnotesize
\begin{enumerate}
  \item Нормируем $X$ так, чтобы $\mu(\mathbf{x_i}) = 0, D(\mathbf{x_i}) = 1$ и $y$, так чтобы $\mu(\mathbf{y}) = 0$
  \item Введем $\beta_1 = 0, r = y$, $A = \emptyset$ --- множество всех направлений с максимальной корреляцией с $r$, $s$ --- вектор знаков корреляций.
  \item $X_A = \left(s_1x_{A(1)}, s_2x_{A(2)}, \ldots s_{|A|}x_{A(|A|)}\right)$
  \item $a_A = \left(1^T (X_A^TX_A)^{-1}1\right)^{-\frac{1}{2}}$
  \item $u_A = a_AX_A(X_A^TX_A)^{-1}1_A$ обратите внимание, что ($X_A^Tu_A = a_A 1_A$)
  \item $c = X^T (y - \beta_t)$
  \item $a = X^T u_A$
  \item $\gamma = min^+_j \left(\frac{C - c_j}{a_A - a_j}, \frac{C + c_j}{a_A + a_j}\right)$
  \item $\beta_{t+1} = \beta_t + \gamma u_A$
\end{enumerate}
\end{frame}

\begin{frame}{Работа LARS (корреляция)}
\begin{center}
\includegraphics[width=0.9\textwidth]{3_14.png} 
\end{center}
\footnotesize
Картинки из The Elements of Statistical Learning (Hastie, Friedman and Tibshirani, 2009)
\end{frame}

\subsection{ISTA}

\section{ElasticNet vs $l_{0.5}$ vs. $l_{1.1}$}
\begin{frame}{Немного о касаниях}
\begin{center}
\includegraphics[height=0.9\textheight]{ElasticNet.png} 
\end{center}
\footnotesize
\begin{itemize}
  \item Хочется получить единственную точку касания
  \item По возможности хотелось бы упростить вычисления
\end{itemize}
Картинки от Hui Zou
\end{frame}

\begin{frame}{ElasticNet, $l_1.1$, etc.}
Можно пойти 2-мя способами:
\begin{enumerate}
  \item Добавить компоненту, которая будет отвечать за ``круглые бока'' (ElasticNet)
  $$
    \arg \min_\beta \|X\beta - y \| + \lambda_1 \|\beta\|_1 + \lambda_2 \|\beta\|_2
  $$
  \item Если жизнь все равно негладкая то гулять так гулять ($1 < q < 2$):
  $$
    \arg \min_\beta \|X\beta - y \| + \lambda \|\beta\|_q
  $$
\end{enumerate}
\end{frame}

\begin{frame}{Grouped LASSO}
Иногда мы априорно знаем, что какие-то переменные похожи. Эту информацию можно использовать:
  $$
    \arg \min_\beta \|X\beta - y\| + \lambda \sum_{g} \sqrt{\sum_{j \in g} \beta_j^2}
  $$
Бывают и другие танцы для того, чтобы обеспечить structural sparsity
\end{frame}

\subsection{Объем данных vs $q$}
\begin{frame}{Как выбирать $q$?}
Есть теоретические работы на эту тему. Не помню кто, не помню как, но доказал, что в ML $q$ зависит от объема выборки. Для малых объемов работает $q = 0$, для бесконечных $q=2$. Все что посередине должны найти свой любимый $q$. \\

\footnotesize
Постараюсь к следующему разу привести чуть более точную ссылку
\end{frame}

\section{Регуляризация}
\begin{frame}{Еще раз о регуляризации}
$$\begin{array}{l}
\arg \max_\beta P(y|X\beta) P(\beta) \\
= \arg \max_\beta \sum_i log P(y_i|x_i\beta) + log P(\beta)
\end{array}$$
Данный фокус гораздо более общий, и подходит не только для линейных решений.
\end{frame}
\end{document}
