\documentclass[14pt, fleqn, xcolor={dvipsnames, table}]{beamer}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{cite,enumerate,float,indentfirst}

\usepackage{tikz}                   
\usetikzlibrary{shadows}

% \usepackage{enumitem}
% \setitemize{label=\usebeamerfont*{itemize item}%
%   \usebeamercolor[fg]{itemize item}
%   \usebeamertemplate{itemize item}}

\graphicspath{{images/}}

\usetheme{Madrid}
\usecolortheme{seahorse}

\setbeamercolor{footline}{fg=Blue!50}
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    И. Кураленок, Н. Поваров, Яндекс
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{}%
    Санкт-Петербург, 2013
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{}%
  Стр. \insertframenumber{} из \inserttotalframenumber \hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt%
}
\newcommand\indentdisplays[1]{%
     \everydisplay{\addtolength\displayindent{#1}%
     \addtolength\displaywidth{-#1}}}
\newcommand{\itemi}{\item[\checkmark]}

\title{Машинное обучение: начало\\\small{}}
\author[]{\small{%
И.~Куралёнок,
Н.~Поваров}}
\date{}

\begin{document}

\begin{frame}
\maketitle
\small
\begin{center}
\vspace{-60pt}
\normalsize {\color{red}Я}ндекс \\
\vspace{80pt}
\footnotesize СПб, 2013
\end{center}
\end{frame}

\section{Формальное описание курса}

\begin{frame}
\frametitle{Что нужно, чтобы понять?}
\begin{itemize}
	\item ТВ и МС
	\item Линейная алгебра
	\item Язык программирования
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Как отчитываться?}
\begin{itemize}
\item К концу обучения сделать 15 минутную презентацию по применению ML в вашей любимой задаче.
\item Задачки
\item Ошибки к лекциях и в слайдам :)
\end{itemize}
\end{frame}

\begin{frame}{Какие у нас цели?}
\begin{itemize}
	\item Уметь сформулировать задачу в терминах ML
	\item Найти подходящий класс решающих алгоритмов по формулировке
	\item Ориентироваться в области и знать ``где посмотреть'' существующие решения
	\item Понимать границы применимости
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Что почитать?}
\begin{itemize}
  \item Википедия (лучше en)
\end{itemize}
\begin{itemize}
  \item T. Hastie, R. Tibshirani, J. Friedman “The elements of Statistical Learning”
  \item T. Mitchell “Machine Learning”
\end{itemize}
\begin{itemize}
  \item Труды конференций: ICML, NIPS, CIKM, KDD, etc.
  \item Журналы: JML, JMLR, JIS, NC, etc.
  \item Видео курс: www.ml-class.org
\end{itemize}
\end{frame}

\section{Введение}

\subsection{Определения}

\begin{frame}
\frametitle{Машинное обучение: определения (I)}
\begin{quote}
A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
\begin{flushright}\textbf{Tom~M.~Mitchell}\end{flushright}
\end{quote}
\end{frame}
\begin{frame}
\frametitle{Машинное обучение: определения (II)}
\begin{quote}
Machine learning --- the ability of a machine to improve its performance based on previous results.
\begin{flushright}\textbf{Webster}\end{flushright}
\end{quote}
\end{frame}
\begin{frame}
\frametitle{Машинное обучение: определения (III)}
\begin{quote}
Машинное обучение —-- обширный подраздел искусственного интеллекта, изучающий методы построения алгоритмов, способных обучаться.
\begin{flushright}\textbf{ru.wikipedia.org}\end{flushright}
\end{quote}
\end{frame}
% Вывод такой: нету единого понимания о чем все это. Также нету структуры, которая была бы стандартной,
% поэтому дальше мы придумали свою собственную.

\subsection{Минутка истории}
\begin{frame}
\frametitle{Машинное обучение: немного истории}
\begin{small}
\begin{description}
	\item[50-70гг] базы знаний, полнотекстовый поиск, распознавание образов, нейронные сети
	\item[70-80гг] символьный вывод, Quinlan ID3 деревья, разумные практические результаты, VC-оценки
	\item[80-90гг] первые конференции, много практического применения, активное применение кластеризации в анализе
	\item[90-00гг] повторное сэмплирование в ML, SVM, применение в IR, ML != DM, LASSO, bootstrap, bagging, boosting
	\item[00-10гг] Compressed sensing и прочие восстановления сигналов, царство деревьев, развитие ансамблей, $\ldots$
\end{description}
\end{small}
\end{frame}

\subsection{Понятия и обозначения}
\begin{frame}
\frametitle{Основные понятия}
\begin{itemize}
	\item Область работы $=$ Universe $=\Gamma$.
	\item Решающая функция $=$ Decision Function $=$ $F_0 \in F$ -- класс решающих функций.
	\item Опыт $=$ Data Set $=$ $D = X \times Y$.
	\item Целевая функция $=$ Target $=T(y,F(x))$.
\end{itemize}
\end{frame}

\begin{frame}{Задача обучения}
В ML оптимизация часто проводится в одних условиях, а эксплуатация в других. 
$$
\arg \max_{F, B: F_0 = B(F)} A(\Gamma, F_0)
$$
\begin{itemize}
	\item $A$ --- цели эксплуатации (например деньги) на всей области работы $\Gamma$
	\item $B$ --- способ оптимизации, который реализуем
\end{itemize}
\end{frame}

\begin{frame}{Как устроена эксплуатация}{На самомделе, как она \_может\_ быть устроена}
Будем считать, что работа на разных элементах $\Gamma$ независима. Тогда эксплуатацию $A$ можно представить так:
$$
A=\mu_{x \sim U(\Gamma)} T_A(F_0(x))
$$
\end{frame}

\begin{frame}{Как устроена оптимизация}{На самомделе, как она \_может\_ быть устроена}
Способ оптимизации $B$, в свою очередь, такая штука:
$$
F_0 = B(F, D) = \arg \max_{F} T_B(D, F) = \arg \max_{F} T_B(Y,F(X));
$$

В рамках этой лекции, мы не будем рассматривать разницу между $T_A$ и $T_B$, поэтому будем считать, что они одинаковы $T$.
\end{frame}

\subsection{Классификация методов ML}
\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item способу получения опыта;
	\item виду целевой функции;
	\item классу решающих функций.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item {\bfспособу получения опыта;}
	\item виду целевой функции;
	\item классу решающих функций.
\end{itemize}
\end{frame}

\subsubsection{В зависимости от способа получения опыта}
\begin{frame}
\frametitle{Классификация машинного обучения: опыт}
\begin{itemize}
	\item Transductive learning
	\item Обычное обучение
	\item Стохастическая оптимизация (stochastic optimization)\footnote{По сути это не обучение, но очень похоже.}
	\item Активное обучение (active learning)
	\item Обучение с бюджетом (budget learning) 
	\item Интерактивное обучение (online learning)
	\item Многорукие бандиты (multi-armed bandits)
	\item Обучение с подкреплением (reinforcement learning) 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Transductive learning}
\begin{enumerate}
	\item Все множество, на котором работаем задано перечислением $\Gamma_0$
	\item Для части данных известен ответ, и они формируют $D$
\end{enumerate}
$$
F_0=B(F, D, \Gamma_0)
$$
\end{frame}

\begin{frame}
\frametitle{Обычное обучение}
\begin{enumerate}
	\item Фиксируем множество примеров $X$
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на доступных примерах, используя информацию о всех
\end{enumerate}
$$
F_0 = B(F, D)
$$
\end{frame}

\begin{frame}
\frametitle{Стохастическая оптимизация\footnote{Одна из возможных версий предмета :).}}
\begin{enumerate}
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на следующем примере пока не надоест (до момента $t_0$)
\end{enumerate}
$$\begin{array}{l}
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \left(x_i \sim U(\Gamma)\right) \\
F_0^t=B(F, D_i) \\
A(F_0) = A(F_0^{t_0}, \Gamma)
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Активное обучение}
\begin{enumerate}
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на всех доступных примерах
	\item Пополняем множество примеров по просьбе алгоритма $\mathcal{A}$ и переходим к п.~2, если $\mathcal{A}$ не требует больше данных останавливаемся в $t_0$
\end{enumerate}
$$\begin{array}{l}
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \{\mathcal{A}(D_t, F_t)\} \\
F_0^t = B(F, D_{t}) \\
A(F_0, \Gamma) = A(F_0^{t_0}, \Gamma)
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Обучение с бюджетом}
\begin{enumerate}
	\item Введем стоимость получения информации по точке $c(d)$ и бюджет обучения $B$
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на всех доступных примерах
	\item Пополняем множество примеров по просьбе алгоритма $\mathcal{A}$, пока не закончился бюджет $\sum_t c({x_t, y_t}) < C$ и переходим к п.~3
\end{enumerate}
$$\begin{array}{l}
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \{\mathcal{A}(D_t, F_t)\} \\
F_0^t = B(F, D_{t}) \\
A(F_0, \Gamma) = A(F_0^{t_0}, \Gamma)
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Интерактивное обучение (online)}
\begin{enumerate}
	\item Определяем генеральную совокупность $\Gamma$
	\item Обучаемся на всех доступных примерах
	\item Получаем следующую точку, исходя из работы решающей функции и переходим к п.~2
\end{enumerate}
$$\begin{array}{l}
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \{(x_{t+1}, y_{t+1})\} \\
F_0^t = B(F, D_{t}) \\
A(F_0, \Gamma) = \sum_{t, x_t \in \Gamma} T_A(y_t, F_0^t(x_t)) \\
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Многорукие бандиты}
\begin{enumerate}
	\item Фиксируем множество возможных действий $M$, $x\in M$.
	\item Каждое действие ведет к $y \sim \xi_x$.
	\item Обновляем модель $\xi_{ti}, \forall i \in M$.
	\item Повторяем с п.3 пока не надоест.
\end{enumerate}
$$\begin{array}{l}
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \{(x_{t+1} \in M, y_{t+1})\} \\
F_0^t = B(F, D_{t}, \xi_t) \\
A(F_0, \Gamma) = \sum_{t, x_t \in \Gamma} T_A(y_t, F_0^t(x_t)) \\
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Обучение с подкреплением\footnote{На самом деле, это уже AI.}}
\small
\begin{enumerate}
	\item Фиксируем множество возможных действий $M$, ``поле'' $X$, множество возможных ответов среды $S = S_o \cup S_n$.
	\item В терминах $S$ сформирулируем целевую функцию $T$.
	\item По опыту $Y \in 2^{S_o}$ научимся прогнозировать весь $S$, а значит и $T$, для точек куда можно перейти;.
	\item Сделаем следующий ход исходя из оптимальности восстановленного состояния.
	\item Обновим $Y$ тем, что нашли в новой точке перейдем к п.3
\end{enumerate}
$$\begin{array}{l}
D_0 = \emptyset \\
D_{t+1} = D_{t} \cup \{(x_{t+1} \in M, y_{t+1})\} \\
F_0^t = B(F, D_{t}, \xi_t) \\
A(F_0, \Gamma) = \sum_{t, x_t \in \Gamma} T_A(y_t, F_0^t(x_t)) \\
\end{array}$$
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item способу получения опыта;
	\item {\bfвиду целевой функции;}
	\item классу решающих функций.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения: цель}
\begin{itemize}
	\item С учителем
	\begin{itemize}
		\item классификация (classification);
		\item аппроксимация (regression);
		\item отношение порядка (learning to rank);
		\item обучение метрики (metric learning).
	\end{itemize}
	\item Без учителя:
	\begin{itemize}
		\item кластеризация (cluster analysis);
		\item уменьшение размерности (dimensionality reduction);
		\item обучение отображению (representation learning).
	\end{itemize}
	\item Смешанные:
	\begin{itemize}
		\item условная кластеризация;
		\item transfer learning.
	\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}[t]\frametitle{Обучение с учителем I}
\begin{description}
	\item[Классификация:] в качестве метрики LL, KL, CE, etc.
	$$\begin{array}{l}
	y \in \left\{-1,1\right\}, F: \Gamma \to \left[0,1\right] \\
	y \in \left\{1,\ldots, m\right\}, F: \Gamma \to \left[0,1\right]^m \\
	y \in \left\{0, 1,\ldots, m\right\}, F: \Gamma \to \left[0,1\right]^{m + 1}\\
	\end{array}$$ 
	\item[Аппроксимация:] варианты MSE.	$y \in \mathbb{R}$, $F: \Gamma \to \mathbb{R}$
\end{description}
\end{frame}
\begin{frame}[t]\frametitle{Обучение с учителем II}
\begin{description}
	\item[Отношения порядка:] цель зависит от модели, но почти всегда хотим найти $F: \Gamma \to \mathbb{R}$:
	\begin{itemize}
	{\setlength{\itemindent}{-4em}
		\item pointwise: $x \in \Gamma, y \in \mathbb{R}, T$ -- MSE; 
		\item pairwise: $x \in \Gamma^2, y \in \left\{<,=,>\right\}, T$ -- см. классификацию;
		\item listwise: $x \in \Gamma^n, y \in ?, T$ -- специфичен для конкретного применения.
	}
	\end{itemize}
\end{description}
\end{frame}
\begin{frame}[t]\frametitle{Обучение с учителем III}
\begin{description}
	\item[Обучение метрики:] хотим построить такую функцию от пары чтобы:
	\begin{itemize}
	{\setlength{\itemindent}{-4em}
		\item Она отражала заданную семантику
	}
	{\indentdisplays{-5em}
		$$\begin{array}{l}
		X=\Gamma^2,\\
		Y = \{(a,b,c,d): m(a,b) < m(c,d), a,b,c,d \in \Gamma\}, \\
		F: \Gamma^2 \to \mathbb{R}
		\end{array}$$
	}
	{\setlength{\itemindent}{-4em}
		\item По возможности была метрикой
	}
	\end{itemize}
\end{description}
Целевая функция обычно основывается на классификации
\end{frame}

\begin{frame}[t]\frametitle{Обучение без учителя I}
\begin{description}
	\leftmargin=-5em
	\itemindent=0em
	\labelwidth=0em
	\leftskip=-5em
	\item[Уменьшение размерности:] надо отобразить исходное пространство в пространство меньшей размерности, максимально сохранив заданные свойства.
	\begin{itemize}
	\leftskip=-5em \small
		\item расстояние: $x,y \in \Gamma, ||F(x) - F(y)|| - ||x - y||$;
		\item статистику: $\{x_i\}_1^n \in \Gamma^n, \Psi(\{x_i\}_1^n) - \Psi(\{F(x_i)\}^n_1)$;
	\end{itemize}
	\item[Кластеризация:] это такое уменьшение размерности до ``упора'', в качестве статистики, которую надо оставить выступает ``чувство прекрасного''. Например: {\indentdisplays{-5em} \small $$
	\Psi = \left\{\begin{array}{l}0, ||x - y|| < \epsilon\\1\end{array}\right.
	$$}
	\item[Обучение представлению] тоже уменьшение размерности, но ограничения накладываются уже на то как видимы результаты.
\end{description}
\end{frame}

\begin{frame}
\frametitle{Классификация машинного обучения}
ML можно делить по:
\begin{itemize}
	\item способу получения опыта;
	\item виду целевой функции;
	\item {\bfклассу решающих функций.}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Основные классы решающих функций}
\begin{itemize}
\item Линейные решения
\item Параметрические семейства функций
\item Графы
\item Нейронные сети (ANN)
\item Instance based learning (kNN)
\item Предикаты
\item Ансамбли
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (I)}
\begin{itemize}
\item Линейные решения 
	\begin{itemize}
	\item Линейная регрессия, логистическая регрессия
	\item Скрытый дискриминантный анализ (LDA/QDA*)
	\item LASSO
	\item SVM
	\item LSI*
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (II)}
\begin{itemize}
\item Графы
\begin{itemize}
	\item Марковские модели (цепи, HMM)
	\item Графические модели
	\item Conditional Random Fields
\end{itemize}
\item Нейронные сети (ANN)
\begin{itemize}
	\item Персептронные сети
	\item Сети Хопфилда++
	\item Сети Кохоннена
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (III)}
\begin{itemize}
\item Параметрические семейства функций
	\begin{itemize}
		\item Сэмплирование
		\item Генетические алгоритмы
		\item PLSI/LDA (Latent Dirichlet Allocation)/прочие модели с распределениями (несть им числа)
	\end{itemize}
\item Instance based learning
	\begin{itemize}
		\item kNN
		\item DANN
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Деление по решающей функции (IV)}
\begin{itemize}
\item Предикаты
	\begin{itemize}
	\item Логические выражения
	\item Деревья решений
	\end{itemize}
\item Ансамбли
	\begin{itemize}
	\item Просто ансамбли 
	\item Bagging
	\item Boosting
	\item BagBoo/BooBag
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}\huge
Вопросы?
\end{center}
\end{frame}

\renewcommand{\arraystretch}{1.5}

\begin{frame}
\frametitle{Дедуктивные/индуктивные методы}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.45\textwidth}|p{0.45\textwidth}}
\rowcolor{Blue!20}
\small Индуктивные & \small Дедуктивные \\
\hline 
Полагаются на статистику & Полагаются на prior knowledge \\ 
Используют классы элементарных функций & Решающая функция следует из предполагаемой структуры \\
Работают в любой области & Привязаны к области \\ 
Знание области отражается на составление target & Понимание области меняет решающую функцию \\
Логистическая регрессия & LDA \\
&\\
\bf \em Для вхождения в область, при больших размерностях & \bf \em Небольшие размерности, «давно тут сидим» \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Data Mining vs. Machine Learning}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}}
\rowcolor{Blue!20}
& \small Data Mining & \small Machine Learning \\
\hline 
\bf Цель дисциплины & Выявление ``скрытых данных'' & Оптимизация целевой функции \\		
\bf Исследования & Больше про данные & Больше про методы \\
\bf Типичный результат & ``Мы применили такой метод и получили клевые результаты на таких стандартных данных'' & ``Предложили новый метод, который работает круче чем другие на нескольких датасетах (возможно даже синтетика)'' \\
\bf Где почитать & SIGIR, WSDM, WWWC, $\ldots$ & ICML, CIKM, $\ldots$ \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Artificial Intelligence vs. Machine Learning}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}}
\rowcolor{Blue!20}
& \small Artificial Intelligence & \small Machine Learning \\
\hline 
\bf Цель дисциплины & Рациональное поведение умных машин & Оптимизация целевой функции \\		
\bf Исследования & Больше про мат. моделирование & Больше про методы \\
\bf Типичный результат & ``Мы придумали как формализовать задачу игры в шахматы, применили такие методы и обыграли человека'' & ``Предложили новый метод, который работает круче чем другие на нескольких датасетах (возможно даже синтетика)'' \\
\bf Где почитать & AAAI, IJCAI, $\ldots$ & ICML, CIKM, $\ldots$ \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Применение методов ML}
\begin{itemize}
\item Практически везде (дайте задачку, я попробую придумать применение)
\item Есть два больших класса работ
\end{itemize}
\begin{center}
\rowcolors{2}{Blue!10}{Blue!5}
\footnotesize
\begin{tabular}{p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}}
\rowcolor{Blue!20}
& \small Академические & \small Практические \\
\hline 
\bf Цели & Существуют ситуации, когда работает хорошо & Обеспечивает измеряемое качество на множестве примеров \\
\bf Искать & Красивые идеи, хорошую математику & Работающие вещи, много грязных приемов \\
\bf Смотреть & Конференции & Соревнования \\
\end{tabular}
\end{center}
\end{frame}

\end{document} 
